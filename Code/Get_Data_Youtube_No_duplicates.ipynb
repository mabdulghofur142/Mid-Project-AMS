{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabdulghofur142/Analisis-Media-Sosial/blob/main/Mid%20Project%20AMS/Get_Data_Youtube_No_duplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d874eb",
      "metadata": {
        "id": "f5d874eb"
      },
      "outputs": [],
      "source": [
        "!pip install google-api-python-client\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192f58b5",
      "metadata": {
        "id": "192f58b5"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# === KONFIGURASI ===\n",
        "api_key = 'Your API'\n",
        "keywords = ['keyword1', 'keyword2']\n",
        "max_results = 3 # maksimum result\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "video_info_list = []\n",
        "top_comments = []\n",
        "replies = []\n",
        "\n",
        "# === FUNGSI ===\n",
        "def search_videos_by_keyword(keywords, max_results=5):\n",
        "    video_ids = []\n",
        "    for keyword in keywords:\n",
        "        search_response = youtube.search().list(\n",
        "            q=keyword,\n",
        "            part='id',\n",
        "            type='video',\n",
        "            maxResults=max_results\n",
        "        ).execute()\n",
        "        video_ids_temp = [\n",
        "            item['id']['videoId']\n",
        "            for item in search_response.get('items', [])\n",
        "            if 'videoId' in item.get('id', {})\n",
        "        ]\n",
        "        video_ids = video_ids + video_ids_temp\n",
        "        print(f\"Ditemukan {len(video_ids_temp)} video untuk keyword '{keyword}'\")\n",
        "    return set(video_ids)\n",
        "\n",
        "\n",
        "def get_video_info(video_id):\n",
        "    response = youtube.videos().list(\n",
        "        part='snippet,statistics',\n",
        "        id=video_id\n",
        "    ).execute()\n",
        "\n",
        "    if not response['items']:\n",
        "        return {}\n",
        "\n",
        "    item = response['items'][0]\n",
        "    snippet = item['snippet']\n",
        "    stats = item['statistics']\n",
        "\n",
        "    return {\n",
        "        'video_id': video_id,\n",
        "        'title': snippet.get('title'),\n",
        "        'description': snippet.get('description'),\n",
        "        'uploader': snippet.get('channelTitle'),\n",
        "        'upload_date': snippet.get('publishedAt'),\n",
        "        'view_count': stats.get('viewCount'),\n",
        "        'like_count': stats.get('likeCount'),\n",
        "        'comment_count': stats.get('commentCount')\n",
        "    }\n",
        "\n",
        "def get_all_replies(parent_id):\n",
        "    all_replies = []\n",
        "    next_page_token = None\n",
        "    while True:\n",
        "        response = youtube.comments().list(\n",
        "            part=\"snippet\",\n",
        "            parentId=parent_id,\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat=\"plainText\"\n",
        "        ).execute()\n",
        "\n",
        "        for r in response.get(\"items\", []):\n",
        "            s = r['snippet']\n",
        "            all_replies.append({\n",
        "                'reply_id': r['id'],\n",
        "                'parent_id': parent_id,\n",
        "                'author': s.get('authorDisplayName'),\n",
        "                'text': s.get('textOriginal'),\n",
        "                'likes': s.get('likeCount'),\n",
        "                'published': s.get('publishedAt'),\n",
        "                'updated': s.get('updatedAt')\n",
        "            })\n",
        "\n",
        "        next_page_token = response.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "    return all_replies\n",
        "\n",
        "def get_all_comments(video_id, cek_replies = True):\n",
        "    next_page_token = None\n",
        "    while True:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat='plainText'\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"items\", []):\n",
        "            try:\n",
        "                top = item['snippet']['topLevelComment']\n",
        "                s = top['snippet']\n",
        "                comment_id = top['id']\n",
        "\n",
        "                top_comment_data = {\n",
        "                    'comment_id': comment_id,\n",
        "                    'video_id': video_id,\n",
        "                    'author': s.get('authorDisplayName'),\n",
        "                    'text': s.get('textOriginal'),\n",
        "                    'likes': s.get('likeCount'),\n",
        "                    'published': s.get('publishedAt'),\n",
        "                    'updated': s.get('updatedAt')\n",
        "                }\n",
        "                top_comments.append(top_comment_data)\n",
        "                if cek_replies:\n",
        "                    all_r = get_all_replies(comment_id)\n",
        "                    replies.extend(all_r)\n",
        "\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "        next_page_token = response.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "# === PROSES UTAMA ===\n",
        "video_ids = search_videos_by_keyword(keywords, max_results=max_results)\n",
        "print(f\"Ditemukan {len(video_ids)} video untuk keyword '{keywords}'\")\n",
        "print(video_ids)\n",
        "for vid in video_ids:\n",
        "    try:\n",
        "        print(f\"Mengambil data video: {vid}\")\n",
        "        info = get_video_info(vid)\n",
        "        if info:\n",
        "            video_info_list.append(info)\n",
        "            get_all_comments(vid, cek_replies=False) # cek_replies=True, Jika ingin mengambil data balasan komen\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat memproses video {vid}: {e}\")\n",
        "\n",
        "# === SIMPAN KE EXCEL ===\n",
        "df_video = pd.DataFrame(video_info_list)\n",
        "df_top = pd.DataFrame(top_comments)\n",
        "df_replies = pd.DataFrame(replies)\n",
        "\n",
        "output_file = \"Filename.xlsx\"\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    df_video.to_excel(writer, sheet_name='VideoInfo', index=False)\n",
        "    df_top.to_excel(writer, sheet_name='TopComments', index=False)\n",
        "    # df_replies.to_excel(writer, sheet_name='Replies', index=False) # aktifkan Jika ingin menyimpan data balasan komen\n",
        "\n",
        "print(f\"âœ… Data disimpan ke file Excel: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0e06e2",
      "metadata": {
        "id": "6d0e06e2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baca file Excel\n",
        "file_path = output_file\n",
        "\n",
        "# Baca masing-masing sheet ke DataFrame\n",
        "df_video = pd.read_excel(file_path, sheet_name='VideoInfo')\n",
        "df_top = pd.read_excel(file_path, sheet_name='TopComments')\n",
        "# df_replies = pd.read_excel(file_path, sheet_name='Replies') # aktifkan Jika sebelumnya menyimpan data balasan komen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885df210",
      "metadata": {
        "id": "885df210"
      },
      "outputs": [],
      "source": [
        "# Fungsi yang digunakan\n",
        "import re\n",
        "import pandas as pd\n",
        "from html import unescape # Import unescape from html module\n",
        "from unidecode import unidecode # Import unidecode\n",
        "\n",
        "def get_teks(teks): # bersihkan teks dari mention dan hashtag - Removed df as it's not used\n",
        "    # Ensure teks is a string before applying regex\n",
        "    if isinstance(teks, str):\n",
        "        teks = re.sub(r'@\\w+','',teks).strip()\n",
        "        teks = re.sub(r'#\\w+','',teks).strip()\n",
        "        return teks\n",
        "    else:\n",
        "        return '' # Return empty string for non-string values\n",
        "\n",
        "def remove_links_email(teks): # besihkan teks dari link dan email\n",
        "    # Ensure teks is a string before applying regex\n",
        "    if isinstance(teks, str):\n",
        "        docx = teks.strip()\n",
        "        urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "        emailPattern = re.compile(r'[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}')\n",
        "        docx = re.sub(urlPattern,' ', docx) # Remove links\n",
        "        docx = re.sub(emailPattern,' ', docx) # Remove email\n",
        "        return docx\n",
        "    else:\n",
        "        return '' # Return empty string for non-string values\n",
        "\n",
        "\n",
        "def unidecode_text(text): # fungsi untuk encode format ASCII dan membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
        "     # Ensure text is a string\n",
        "    if isinstance(text, str):\n",
        "        return unescape(unidecode(text))\n",
        "    else:\n",
        "        return '' # Return empty string for non-string values\n",
        "\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Handle potential NaN values in the 'text' column by replacing them with empty strings\n",
        "df_top['text'] = df_top['text'].fillna('')\n",
        "\n",
        "df_top['teks_new'] = df_top['text'].apply(lambda x: get_teks(x))\n",
        "df_top['teks_new'] = df_top['teks_new'].apply(lambda x: remove_links_email(x))\n",
        "df_top['teks_new'] = df_top['teks_new'].apply(lambda x: unidecode_text(x))\n",
        "df_top['teks_new_lower'] = df_top['teks_new'].apply(lambda x: lower(x))\n",
        "df_top.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f43d90",
      "metadata": {
        "id": "c1f43d90"
      },
      "outputs": [],
      "source": [
        "df_top_uniq = df_top.drop_duplicates(subset=['author', 'teks_new_lower'], keep='first')\n",
        "# df_top_uniq = df_top_uniq.drop(columns=['teks_new', 'teks_new_lower'])\n",
        "df_top_uniq.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517b4823",
      "metadata": {
        "id": "517b4823"
      },
      "outputs": [],
      "source": [
        "output_file = \"Filename_uniq.xlsx\"\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    df_video.to_excel(writer, sheet_name='VideoInfo', index=False)\n",
        "    df_top_uniq.to_excel(writer, sheet_name='TopComments', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f940e6ae",
      "metadata": {
        "id": "f940e6ae"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}